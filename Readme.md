# ğŸ§  Image Caption Generator Using CNN-LSTM (FastAPI Deployment)
## ğŸ“˜ Overview

This project implements an Image Caption Generator, a deep learning model that automatically generates descriptive captions for images.
It combines Convolutional Neural Networks (CNNs) for image feature extraction and Recurrent Neural Networks (LSTMs) for language modeling, trained on imageâ€“caption datasets.

You can upload an image via a FastAPI web interface, and the app returns a meaningful caption generated by the trained model.
---
## ğŸš€ Project Workflow
### 1. Data Collection & Preprocessing

- Dataset Used: Flickr8k Dataset (8,000 images with 5 captions each).

- Preprocessing Steps:

Cleaned captions (removed punctuation, lowercase conversion, tokenization).

Added special tokens: "start" and "end" to each caption.

Used InceptionV3 (pretrained on ImageNet) for feature extraction.

Extracted 2048-dimensional feature vectors for each image.

### 2. Text Tokenization

- Used Tokenizer from Keras to build a vocabulary from all captions.

- Converted captions to integer sequences.

- Applied padding to make all sequences of equal length.

- Defined max_length based on the longest caption.

### 3. Model Architecture

The model follows a CNN + LSTM Encoderâ€“Decoder approach.

ğŸ§© Encoder (Image Feature Extractor)

Input: Extracted feature vector (2048-dim).

Layers:

Dropout(0.5)

Dense(256, activation='relu')

Output: 256-dim projected feature.

## ğŸ§  Decoder (Language Model)

Input: Sequence of tokens.

Layers:

Embedding(vocab_size, 256, mask_zero=True)

LSTM(256, return_sequences=True)

Dense(vocab_size, activation='softmax')

## ğŸ—ï¸ Combined Model

The encoder and decoder outputs are merged via add(), followed by Dense layers to predict the next word in the sequence.

## ğŸ‹ï¸ Training

Epochs: 17

Batch Size: 32

Optimizer: Adam

Loss: Categorical Cross-Entropy

Used data generator to feed (image features, input sequence, output word) tuples in memory-efficient batches.

Validation captions were generated at intervals to monitor quality.

## ğŸ§© Model Evaluation

Generated captions for random test images.

Actual:
- two dogs are playing with each other on the pavement  
- black dog and tri-colored dog playing with each other on the road  

Predicted:
- two dogs are playing on the road

---
## âš™ï¸ FastAPI Deployment
1. Backend (main.py)

Built REST API using FastAPI.

Endpoint /predict/ accepts uploaded image and returns generated caption.

Utilized pre-trained model (model.h5), tokenizer (tokenizer.pkl), and features extractor.

2. Frontend

Simple and elegant HTML + CSS form.

Upload an image â†’ get caption â†’ view output instantly.

Deployed locally via:

```bash
uvicorn main:app --reload
```
# vâœ¨ Author

Ali Ahmad

Data Scientist & AI/ML Engineer

ğŸ“§ aliahmaddawana@gmail.com